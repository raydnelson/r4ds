---
title: "HW21"
author: "Ray Nelson"
date: "2 Apr 2019"
output:
  html_document:
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
```

## Concepts  

Compare and constrast the machine learning and the classical modeling approaches.
    
## Interpretation of code  

Interpret the following R code chunks: 

Code Chunk #1 Heat map of natural log transformed price and carat variables

```{r heat map of natural logarithm of diamond prices and weights}
diamonds2 <- diamonds %>% 
  filter(carat <= 2.5) %>% 
  mutate(lprice = log(price),
         lcarat = log(carat)
         )

ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50)
```

Code Chunk #2 Comparison of predicted with actual data when when we have a log transformation

```{r Comparison of actual and predicted valued}
mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)

grid <- diamonds2 %>% 
  data_grid(carat = seq_range(carat, 20)) %>% 
  mutate(lcarat = log(carat)) %>% 
  add_predictions(mod_diamond, "lprice") %>% 
  mutate(price = exp(lprice))

ggplot(diamonds2, aes(carat, price)) + 
  geom_hex(bins = 50) + 
  geom_line(data = grid, colour = "red", size = 1)
```

## Write Code

Estimate the following linear model and create a boxplot of the residuals by cut:

lm(log(price) ~ log(carat) + cut + color + clarity, data = diamonds)

Do you think that there is any additional information left in the residuals? Why?

```{r diamonds prediction model}

```